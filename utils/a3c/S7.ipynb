{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "We build a program that can play video games:\n",
    "```\n",
    "Input: Game-Environment (env)\n",
    "Output: Game-Policy (policy)\n",
    "```\n",
    "\n",
    "__env__:\n",
    "```\n",
    "Input: action (0~k, say, 2)\n",
    "Output: screen-image, reward, game-is-over\n",
    "```\n",
    "\n",
    "__policy__:\n",
    "```\n",
    "Input: screen-image\n",
    "Output: action\n",
    "```\n",
    "\n",
    "It is not difficult to imagine how a policy “plays” an env. The goal is to design the policy self-inspection and adjustment scheme (call this meta-policy if you like), so the total reward it receives in a game maximises. Note this setting can be more generic than you might think of:\n",
    "- the game can give a reward of any constant positive value at each step to simply encourage the player to stay playing as long as possible, which makes sense in some balancing or jumpping games. In some games that can even mean disencourage winning, such as in ball games!\n",
    "- the game can give a reward of any constant negative value at each step to encourage quick playing, such as in a maze game without suicidal option, this is equal to saying \"hurry up!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "1. Neural Networks\n",
    "    1. Network building using torch\n",
    "    2. Network training using gradient descent\n",
    "        1. Compute gradients with back propagation\n",
    "        2. Commit parameter update along gradient direction\n",
    "        \n",
    "2. Reinforcement Learning Algorithms\n",
    "    1. Long-term evaluation of actions -- building a slot-machine playing agent\n",
    "    2. Handling machines with internal states\n",
    "    3. Handling machines with MANY internal states\n",
    "        1. Using neural networks to estimate action values\n",
    "        2. Adjust action probability by reviewing consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Terms\n",
    "\n",
    "A small number of technical terms are used in our discussion. One way to treat a strange jargon is just ignore it when encountering, and let its meaning emerge by itself during your study. If you find your short-term memory is going to explode because the need to keep track many strange notions -- it might be helpful to look up in a glossary such as [here](https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/). Or simply Google the new concept. However, I am afraid google/wiki-def of the notion can only partially help your study -- the key is the fact that many new concepts need to be learned, so only careful review can help achieve a deep understanding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "You should be able to handle the homeworks left in the prior studio. E.g. after we have studied the \"exploitation and exploration\" in slot-machine playing. You should be able to construct a slot-machine player, from the template we discussed on class.\n",
    "\n",
    "Most importantly, you need to be able to modify and test code template provided to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Glossary\n",
    "\n",
    "You need to understand the relationship between matrix-vector production and its correspondence to linear equation systems; derivation of simple computations and common functions, $d e^x/dx = e^x$ for example.\n",
    "\n",
    "Some math representation of useful concepts are:\n",
    "\n",
    "- $s_t$: the state observed at time $t$. This is usually but NOT always the stuff returned to the agent at the time taking actions. The most prominent exception is the screen-image-based states in our video-game playing examples. One applies some simple preprocessing to the states.\n",
    "\n",
    "- $a_t$: the action taken at time $t$. Generally, it is an integer $\\{0, 1, ..., K-1\\}$ if there are $K$ different actions. Keep in mind that the actual action could be represented differently, such as \"press A-button\". For a decision making agent, given all possible action choices, choosing actions is eqivalent to choosing the indexes.\n",
    "\n",
    "- $r_t$: the immediate reward received at time $t$. Note some authors used to let $r_t$ refer to the reward received __after__ taking action $a_t$ in state $s_t$, while others take $r_t$ as the reward received __at the beginning__ at time $t$, after taking action $a_{t-1}$ in state $s_{t-1}$. In whatever way, the procedure: in $s_t$ taking action $a_t$ according to some policy $\\pi$ arriving the next state $s_{t+1}$ and receiving a reward $r_{t}$ (or $r_{t+1}$ subject to your choice of denotation) is called a __transition step__.\n",
    "\n",
    "- $\\pi(\\cdot|s)$, given the state $s$, a policy $\\pi(a|s)$ (not to be confused with the $\\pi\\approx3.1416$) assigns a non-negative real number to each action -- it dictates the possibility of choosing the action given $s$. If a policy is deterministic, rather than stochastic, it can attribute all probabilities to one particular action, so that the corresponding $\\pi(a|s)=1$ and $\\pi(a'|s)=0$ for all other actions $a'$.\n",
    "\n",
    "- $Q^\\pi(s, a)$, evaluation of __long term__ return for taking action $a$ in state $s$. Since it considers future effects, it relies on the on-going policy, $\\pi$. Note taking $a$ at the current state $s$, the very first step of this evaluation is not necessarily with respect to $\\pi$. Consider this $Q$-evaluation as answering a hypothetical question: what the long term reward would have been if she took action $a$ at $s$ and followed $\\pi$ henceafter. Of course if this evaluation is known, it is wise to take the action maximising this $Q$ at each $s$.\n",
    "\n",
    "- Note in neural network implementation, $Q$- and $\\pi$-nets share the same structure: map states to $K$ numbers, where $K$ is the number of actions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Style\n",
    "\n",
    "We follow python code style specified by [PEP8](https://www.python.org/dev/peps/pep-0008/). You can install PyCharm (free for students) to help you ensure your code complies.\n",
    "\n",
    "Good code should be self-explaining for technical operations, e.g.\n",
    "\n",
    "```\n",
    "action_probabilities = policy(state)\n",
    "rand_hit = rand()\n",
    "accumulated_probability = 0\n",
    "for a_, p_ in zip(actions, action_probabilities):\n",
    "    accumulated_probability += p_\n",
    "    if rand_hit <= accumulated_probability:\n",
    "        chosen_action = a_\n",
    "        break\n",
    "```\n",
    "\n",
    "The individual steps should be clear enough. Comments explaining the design logic may be added if that code is an essential part\n",
    "```\n",
    "# Randomly choose an action according to the policy.\n",
    "# rand_hit is in [0, 1], we start from 0, accumulating the probability of \n",
    "# one action after another, at the moment rand_hit is exceeded, that action\n",
    "# is our choice.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful libraries\n",
    "\n",
    "Mainly we will use ```numpy, pytorch, gym``` for the core functionalities. Several often used utility libraries includes ```PIL, os, collections, itertools, matplotlib, time```. Note some libraries come with sub-modules that need independent import, such as ```torch.nn```. \n",
    "\n",
    "Some libraries are imported for some particular purpose, e.g. you can save a collection of parameters in a dictionary as a file using ```json```. However, the functionality can be easily replaced using ```torch.save```. Those are non-essential dependencies, you can choose not to use those libraries if they cause trouble on a particular computer environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net-building in torch\n",
    "\n",
    "Pytorch provides convenient building blocks for deep neural networks. All processing stages that accept information in some predefined form and output in another predefined form are of the same kind: `module`. Note the notion definition is recursive. If we combine two modules, A and B, so that the information is processed by A, while the middle product is fed to B producing the final product, the combined operation \"Process by A then B\" is itself a processing step albeit a more complex one, and therefore belongs to the `module` class as well. In this way, one can make complex, reusable modules.\n",
    "\n",
    "Pytorch contains many commonly used modules:\n",
    "- [linear modules](http://pytorch.org/docs/master/nn.html#linear-layers) for generic mapping, it is the rule-of-thumb choice when you know the input data has $m$ attributes and the output has $n$.\n",
    "- [convolutional modules](http://pytorch.org/docs/master/nn.html#convolution-layers) for dealing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "=> no checkpoint found at 'True'\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/projects_A/1 DRL Studio/LearningMaterials/W4/a3c.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/toolbox/anaconda3/envs/tc/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/toolbox/anaconda3/envs/tc/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/toolbox/anaconda3/envs/tc/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run a3c.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# this is what happened at time t\n",
    "v = net(im)\n",
    "v[0], v[1], v[2]\n",
    "0.9   0.08  0.02\n",
    "a is 0\n",
    "\n",
    "at T: we have reviewed the episode, and found: \n",
    "at t, we were doing better than average\n",
    "\n",
    "what is the value you hope v to have next time if we observed im:\n",
    "v[0] v[1] v[2]\n",
    "0.92 0.05, 0.03\n",
    "\n",
    "```\n",
    "so we want to change net.parameters, so to get action 0 (the action I took at time $t$, that means I need to keep record of actions)\n",
    "\n",
    "$\\nabla_\\theta \\log p(s, a)$ -- the direction along which I adjust net.parameters `log net(im)[0]` will increase.\n",
    "\n",
    "$\\nabla_\\theta \\log p(s, a) A(s, a) $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
