{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTING IMPLEMENTATION\n",
    "\n",
    "22 Aug 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[batch norm](https://www.youtube.com/watch?v=nUUqwaxLnWs) and [for testing](https://www.youtube.com/watch?v=5qefnAek8OA).\n",
    "\n",
    "A diagnostic analysis of network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from models.darknet import Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for create modules\n",
    "bcks = parse_cfg(\"cfg/yolov3.cfg\")\n",
    "net = create_modules(bcks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet = Darknet(\"cfg/yolov3.cfg\")\n",
    "weight_file = 'checkpoints/yolov3.weights'\n",
    "dnet.load_net_binary(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(dnet.module_list)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = input_images[0]\n",
    "r_ = 200\n",
    "c_ = 300\n",
    "print(batch[0, 0][r_:r_+3, c_:c_+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det0 = dnet(input_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging snippet for loading parameters.\n",
    "for m_ in dnet.module_list:\n",
    "    break\n",
    "    \n",
    "ps = list(m_.parameters())\n",
    "p1 = list(m_[1].parameters())\n",
    "p2 = list(m_[2].parameters())\n",
    "\n",
    "def f_(pl):\n",
    "    print(len(pl))\n",
    "    for p in pl:\n",
    "        print(p.shape)        \n",
    "f_(ps)\n",
    "print('----')\n",
    "f_(p1)\n",
    "print('----')\n",
    "f_(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIT TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.rand(2, 255, 10, 10)\n",
    "inp_dim = 320\n",
    "anchors = ((116,90),  (156,198),  (373,326))\n",
    "ncls = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = predict_transform(pred, inp_dim, anchors, ncls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 608, 608)\n",
    "import time\n",
    "t0 = time.time()\n",
    "y = dnet(x)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ims = [cv2.imread(x) for x in imlist]\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = cv2.imread('imgs/giraffe.jpg')\n",
    "im2 = cv2.imread('imgs/eagle.jpg')\n",
    "loaded_ims = [im1, im2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_batches = list(map(prep_image, loaded_ims, [412, 412]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dnet(im_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def show(x):\n",
    "    plt.imshow(x.detach().numpy().transpose([1, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    \"\"\"\n",
    "    #Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "    \n",
    "    #get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    #Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) \\\n",
    "               * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "\n",
    "    #Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def write_results(pred, confid_threshold, num_classes, nms_conf=0.4):\n",
    "    conf_mask = (pred[:,:,4] > confid_threshold)\n",
    "    batch_size = pred.size(0)\n",
    "    batch_boxes = []\n",
    "    batch_classes = []\n",
    "    batch_scores = []\n",
    "    for i in range(batch_size):\n",
    "        p_ = pred[i]\n",
    "        det_indexes = conf_mask[i].nonzero().squeeze()\n",
    "        if det_indexes.numel() == 0: continue\n",
    "        p_ = p_[det_indexes]\n",
    "        box_corner = torch.zeros_like(p_)\n",
    "        box_corner[:,0] = (p_[:,0] - p_[:,2]/2.0)\n",
    "        box_corner[:,1] = (p_[:,1] - p_[:,3]/2.0)\n",
    "        box_corner[:,2] = (p_[:,0] + p_[:,2]/2.0)\n",
    "        box_corner[:,3] = (p_[:,1] + p_[:,3]/2.0)\n",
    "        boxes = box_corner[:,:4]\n",
    "        scores, clss = torch.max(p_[:,5:], dim=1)\n",
    "        \n",
    "        for c_ in torch.unique(clss):\n",
    "            c = c_.item()\n",
    "            cindex = (clss==c).nonzero().squeeze()\n",
    "            nc = cindex.numel()\n",
    "            for i1_ in range(nc-1):\n",
    "                i1 = cindex[i1_]\n",
    "                i2s = cindex[i1_+1:]\n",
    "                if scores[i1]<confid_threshold: # if alread supressed?\n",
    "                    continue\n",
    "                # supress those with much overlapping\n",
    "                ious = bbox_iou(boxes[i1].unsqueeze(dim=0), boxes[i2s])\n",
    "                supress_ind = i2s[(ious > nms_conf).nonzero().squeeze()]\n",
    "                scores[supress_ind] = 0\n",
    "                # print(i1, ious)\n",
    "                # print('\\t', supress_ind)\n",
    "        im_valid_det_ind = (scores > confid_threshold).nonzero().squeeze()\n",
    "        # 3 tensors per image in batch\n",
    "        batch_boxes.append(boxes[im_valid_det_ind])\n",
    "        batch_classes.append(clss[im_valid_det_ind])\n",
    "        batch_scores.append(scores[im_valid_det_ind]) \n",
    "    return batch_boxes, batch_classes, batch_scores\n",
    "\n",
    "boxes, clss, scs = write_results(pred, 0.2, 80, nms_conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_box(x, results):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    img = results[int(x[0])]\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2, color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), \n",
    "                cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "import os\n",
    "test_dir = 'data/test_imgs'\n",
    "class_names = load_classes(\"data/coco.names\")\n",
    "image_names = [os.path.join(test_dir, n_) for n_ in os.listdir(test_dir)]\n",
    "images = list(map(cv2.imread, image_names))\n",
    "input_images = [prep_image(im_, 416) for im_ in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det0 = dnet(input_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det0[0][:,:4][(det0[0][:,4]>0.5).nonzero().squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(input_images[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detection results are like \n",
    "```\n",
    "tensor([[   3.8977,    6.6006,  230.1316,  216.2989],\n",
    "        [   4.2886,    6.5419,  257.9221,  227.7933],\n",
    "        [   4.4089,    6.5725,  256.6898,  241.6108],\n",
    "        [   5.2665,    6.5663,  220.9306,  234.7696],\n",
    "        [   5.5405,    6.5900,  333.2379,  219.7512],\n",
    "        [   8.7730,    6.9347,  155.0774,  199.1949],\n",
    "        [   9.1378,    6.9157,  146.5729,  202.6583],\n",
    "        [   8.7669,    7.0903,  158.5159,  196.2266],\n",
    "        [   9.1075,    7.1043,  147.2384,  196.7485],\n",
    "        [  24.9396,   17.3632,   28.8222,   70.3265],\n",
    "        [  25.0647,   17.4225,   27.7167,   71.3745]])\n",
    "```\n",
    "\n",
    "I think this is not normal -- most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_box(out_im, box, c, score, class_names, input_size=416):\n",
    "    raw_h, raw_w = out_im.shape[:2]\n",
    "    scaling_factor = min(float(input_size)/raw_h, float(input_size)/raw_w)\n",
    "    box[:,[1,3]] -= (input_size - scaling_factor*raw_w)/2.0\n",
    "    box[:,[0,2]] -= (input_size - scaling_factor*raw_h)/2.0\n",
    "    box /= scaling_factor\n",
    "    box[:,1].clamp_(0.0, raw_w)\n",
    "    box[:,3].clamp_(0.0, raw_w)\n",
    "    box[:,0].clamp_(0.0, raw_h)\n",
    "    box[:,2].clamp_(0.0, raw_h)\n",
    "    \n",
    "    for b_, c_, s_ in zip(box, c, score):\n",
    "        c1 = tuple(b_[:2].int())\n",
    "        c2 = tuple(b_[2:4].int())\n",
    "        color = (0,255,0)\n",
    "        label = \"{0}\".format(class_names[c_.item()])\n",
    "        cv2.rectangle(out_im, c1, c2, color, 1)\n",
    "        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "        c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "        cv2.rectangle(out_im, c1, c2, color, -1)\n",
    "        cv2.putText(out_im, label, (c1[0], c1[1] + t_size[1] + 4), \n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    \n",
    "    print(box)\n",
    "    return out_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box0, cls0, score0 = write_results(det0, 0.5, 80)\n",
    "oim = mark_box(images[0].copy(), box0[0], cls0[0], score0[0], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(oim[:,:,(2,1,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(input_images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = torch.load(os.path.expanduser(\"/Users/junli/tmp/yolotut_105-convolutional.pt\"))\n",
    "#t2 = torch.load(os.path.expanduser(\"/Users/junli/tmp/yolomy_105-convolutional.pt\"))\n",
    "t1 = torch.load(os.path.expanduser(\"/Users/junli/tmp/yolotut_094-yolo.pt\"))\n",
    "t2 = torch.load(os.path.expanduser(\"/Users/junli/tmp/yolomy_094-yolo.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t1-t2).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t1[0, :, 2:]-t2[0, :, 2:]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[0][:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[0][:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dnet.module_list)[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(13)\n",
    "a, b = np.meshgrid(grid, grid)\n",
    "\n",
    "x_offset = torch.FloatTensor(a).view(-1, 1)\n",
    "y_offset = torch.FloatTensor(b).view(-1, 1)\n",
    "\n",
    "x_y_offset = torch.cat(\n",
    "        (x_offset, y_offset), 1).repeat(1, num_anchors)\n",
    "    x_y_offset = x_y_offset.view(-1, 2).unsqueeze(0)\n",
    "    pred[:, :, :2] += x_y_offset\n",
    "    print(pred.shape, x_offset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在改动和学习 Yolo Detector 过程中，我想把例子中的固定大小的输入图像改成对图像大小不做要求（Fully Convolutional 家族的网络本来都有这个潜质）。所以在“翻译” final convolution layer 输出的时候，我把图像的高传入了这个整合 tensor 的函数。\n",
    "\n",
    "```\n",
    "# in forward function\n",
    "...\n",
    "    def forward(self, x)\n",
    "        ...\n",
    "        elif mod == 'yolo':\n",
    "            x = transform_prediction(..., x.shape[2], ...)\n",
    "```\n",
    "但是发生了很诡异的检测结果。给定输入图像有反应，但是输出形状看上去合理却毫无意义的检测框。\n",
    "\n",
    "把原tutorial重新跑了一遍，结果是对的。所以我就\n",
    "1. 检查两个程序中的输入图像（处理成Tensor以后）是不是一致 （结果：是）\n",
    "    - 检查两个程序中的量是不是一致只能分别写保存这些变量的子程序，再从第三个独立的笔记本里面把保存的变量读取出来比较。\n",
    "2. 每一个网络 block 的输出是不是一致 （第一层就不一致）\n",
    "3. 检查第一层block （```nn.Sequential``` 对象）的三个组件分别的输出是不是一致：结果发现convolution 层完全没有问题，但是过了 batch normalisation 就不一致了。\n",
    "4. 检查 bn 层的参数是否一致，结果发现两个程序的bn层的4个参数确实一致。\n",
    "5. 再读了一遍 Ioffe 2015 的paper，确实推导了一下 BN 的工作原理 顺便“发现了” google brain 今年做的一个很漂亮的工作：为了证实 ResNet 的 short cut 是否工作，使用分析每层的统计特征的方式发展了一项网络初始化的方法 (Xiao et al. 2018, Dynamical Isometry and Mean Field Theory of CNNs) -- 我也“顺便”复习了一下mean-field, variational inference in exponential family。\n",
    "6. 毫无头绪下放弃。过后再次思考突然想起，是不是 bn 保存的时候会“残存”训练至此的样本var和mean。发现我忘记设置自己的网络是evaluation而不是training模式了。\n",
    "7. 设置好eval模式后，网上下载的程序和我自己的程序里面的网络的头40个blocks都保持完全的一致。\n",
    "8. 经过第一个upsampling发现结果又不一致了，原来是upsample的插值方法不一样。修正后两个网络头80层完全一致。\n",
    "9. 但是我仍然感觉有问题，因为仅仅是train/eval的区别不应该导致结果的质变。\n",
    "10. 果然在yolo层之后网络的输出又不一致了。但神奇的是，只有82/94/106这三个yolo层不一致，所有的convolution层都没问题。即问题出在对预测的解释上，不是预测出了问题。\n",
    "11. 研读不一致的输出发现这样的patterns\n",
    "```\n",
    "In [38]: (t1[0, :, 2:]-t2[0, :, 2:]).abs().max()\n",
    "Out[38]: tensor(0.)\n",
    "In [39]: t1[0][:10, :2]\n",
    "Out[39]:\n",
    "tensor([[ 11.1450,   9.9402],\n",
    "        [  7.7179,   9.6228],\n",
    "        [ 10.2525,   5.8548],\n",
    "        [ 24.0455,  11.3283],\n",
    "        [ 26.1821,  12.6868],\n",
    "        [ 24.8711,   6.4923],\n",
    "        [ 39.8953,   9.5904],\n",
    "        [ 39.9461,   9.9147],\n",
    "        [ 38.7679,   5.9747],\n",
    "        [ 56.5952,  10.5661]])\n",
    "In [40]:t2[0][:10, :2]\n",
    "Out[40]:\n",
    "tensor([[ 0.6966,  0.6213],\n",
    "        [ 0.4824,  0.6014],\n",
    "        [ 0.6408,  0.3659],\n",
    "        [ 1.5028,  0.7080],\n",
    "        [ 1.6364,  0.7929],\n",
    "        [ 1.5544,  0.4058],\n",
    "        [ 2.4935,  0.5994],\n",
    "        [ 2.4966,  0.6197],\n",
    "        [ 2.4230,  0.3734],\n",
    "        [ 3.5372,  0.6604]])\n",
    "```\n",
    "即解释预测的位置部分出了问题。这部分代码如下\n",
    "```\n",
    "# Add the center offsets\n",
    "    grid = np.arange(grid_size)\n",
    "    \n",
    "    pred[:, :, :2] += x_y_offset\n",
    "    ...\n",
    "    pred[:, :, :4] *= stride\n",
    "```\n",
    "其中 offsets 只不过是grid的节点坐标。检查无误。最终发现是stride（本层的每个检测方块的大小）出了问题：\n",
    "```\n",
    "stride = input_dim // pred_height\n",
    "```\n",
    "其中 input_dim 应该是我的原始图像大小，但是这里却正好跟我的 pred_height (上一层的输出大小）一样，stride总是1. 最终原因是\n",
    "\n",
    "> 使用 x.shape 的时候我错误的以为这是整个forward 函数的x参数，即输入图像。但是其实是我上一层输出的结果。因为我们错误的命名了变量，中间处理的结果应该约定俗成为h或者z什么的，这里因为使用了循环，就造成图一时方便，使用 `x = block[i](x)` 的形式。\n",
    "\n",
    "一些教训和学到的东西：\n",
    "- 修正这个bug耗费了我2个工作日的大部分整块时间和一些零碎时间。总计约20小时。对于一个变量名来说，可谓一字千金。\n",
    "- 幸运和不幸的是 `pred[2:4]`，即框的大小也受 stride 影响，然而它们还要乘以 `anchors`，which are inversely affected by stride, 两厢抵消了，所以检测框大小正常（否则早就怀疑到这个乘数上了，但是也会错过检查出batch normalisation和upsample的问题，以及下面学习的一些经验）。\n",
    "- 能够解剖模型级别的动手能力是必要的。动手能力还欠缺一点的学习者可能在上述Step 2就卡住，从而得到 Yolo 在自己的问题上不好用的结论打退堂鼓了。\n",
    "- 从我的研究得到一个直观感觉。Yolo类别的检测器设计很有趣，其输出检测的层乃是从一系列convolution流水线上取样下来输出（从而只是反馈loss，并不影响计算流水线进一步工作），这样的设计是否可以跟RL的思想结合起来，比如在前序的输出confidence比较高的时候，喂给模型更近一步的信息（e.g.，early step的检测输出可以作为“是否要求zoomed-in的高分辨率数据”的决策）（中举和安成组）。\n",
    "- Exponential family and graphical model是个思想上的treasure box，应该学而时习之。Data statistics (mean/variance)的迁移及其应对理论应该可以用到我们下面的Representation的工作上（安成组）。\n",
    "- Yolo的解释如果换一下，也许可以比较直接地使用在人体部位和姿态的估计上，因为解释层只是重新安排调整convolution的输出的一个分支，即使很复杂也不会特别影响下面的运算（黄韬组）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2\n",
    "import visdom\n",
    "from models.darknet import Darknet, write_results\n",
    "from data import prep_image, load_classes\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "viz = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet = Darknet(\"cfg/yolov3.cfg\")\n",
    "weight_file = 'checkpoints/yolov3.weights'\n",
    "dnet.load_net_binary(weight_file)\n",
    "dnet.eval()\n",
    "\n",
    "test_dir = 'data/test_imgs'\n",
    "class_names = load_classes(\"data/coco.names\")\n",
    "image_names = [os.path.join(test_dir, n_) for n_ in os.listdir(test_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_images = list(map(cv2.imread, image_names))\n",
    "input_images = [prep_image(im_, 416) for im_ in cv_images]\n",
    "cv_img_to_tensor_prep = ToTensor()\n",
    "raw_ims = [prep(im_)[[2, 1, 0]] for im_ in cv_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "pred = dnet(input_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners, clss, scores = write_results(pred, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 256.4855,   97.7296,  373.8611,  144.6448],\n",
       "         [  62.9805,  103.2917,  307.9106,  273.8806],\n",
       "         [  69.6696,  173.8666,  168.1434,  327.4778]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 473.5117,   84.4239,  690.2051,  171.0365],\n",
      "        [ 116.2717,   94.6924,  568.4503,  409.6256],\n",
      "        [ 128.6208,  224.9844,  310.4185,  508.5743]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "oim = mark_box(cv_images[0].copy(), corners[0], clss[0], scores[0], class_names)\n",
    "oim_ = prep(oim)[[2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imwnd0 = viz.image(raw_ims[0], \n",
    "                   opts=dict(title='Random!', caption='How random.'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imwnd0 = viz.image(oim_, opts=dict(title='Random!', caption='How random.'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Liu et al., given $x_1$ in Mode-1, the constraint on the corresponding $z_1$ is that, \n",
    "    - $z_1 \\sim \\mathcal{N}(0, \\mathbf{I}_q) $ (say, $z \\in \\mathbb{R}^q$)\n",
    "    - $z_1$ contains sufficient information about $x_1$ so that the decoder $D_1$ can recover the original signal $D_1: z_1 \\mapsto \\tilde{x}_1 \\simeq x_1$\n",
    "    - This is a bit misled ...\n",
    "    \n",
    "- in eq. 2, there are constraints / penalty on the discrimition on translated/generated images and the original one $L_{GAN_1}, L_{GAN_2}$, and cycle consistency $L_{CC_1}, L_{CC_2}$ -- there is an issue in those images: if more than two modes are considered, it is difficult to construct such constraints, because they are pair-wise, and the number of loss terms would be in the order of $\\mathcal{O}(k^2)$, where $k$ is the number of modes.\n",
    "\n",
    "- so if we can impose constraint on $z$, so that the encoding from different modes are \"mixed\", it is possible to use the unified penalty for any multi-modality transformation.\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
