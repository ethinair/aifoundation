{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This Talk\n",
    "\n",
    "- Problem definition -- AI\n",
    "- The main framework of learning\n",
    "- Knowledge roadmap\n",
    "- More Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Intelligence\n",
    "\n",
    "## What is AI\n",
    "\n",
    "- What makes AI \n",
    "    - Or may we start with: what looks like AI but does NOT stand scrutinise.\n",
    "    - __Q__: what is the more challenging problem: auto-pilot from Sydney to Singpore, or auto-driving from School to Shopping?\n",
    "    - Decision making machines: check this [very short introduction on robots (chinese)][AIClip].\n",
    "[AIClip]:https://www.bilibili.com/video/av28946374/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Modelling the world (and hopefully do something about it)\n",
    "    - Models: idelly, generally applicable patterns\n",
    "        - Can we build simple rules like what Archimedes did for geometry (or Newton for classical physics)?\n",
    "            - Yes! why not trying? Perheps that what the idea on computers by the first programmer! \n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a4/Ada_Lovelace_portrait.jpg\" alt=\"Ada\" height=\"200\" width=\"100\">\n",
    "\n",
    "> [The Analytical Engine] might act upon other things besides number, were objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations, and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engine...Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Unfortunatelly, things do not like go this way -- check [here][Minsky] for a powerful argument from Minsky.\n",
    "    - Intelligent brains is the result of hundreds of millions years of construction by evolution - so you'd expect more structures\n",
    "    - EXAMPLE-INTRO\n",
    "[Minsky]:https://youtu.be/RZ3ahBm3dCk?t=1m28s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXAMPLE -INTRO\n",
    "\n",
    "Consider the data of three numbers and an associated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def triarea(a, b, gamma):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a: edge of one edge\n",
    "    :param b: edge of another\n",
    "    :param gamma: angle\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return a * b * torch.sin(gamma/180.0*3.1416)\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "#from matplotlib.collections import PatchCollection\n",
    "\n",
    "def draw_sample(sample, ax):\n",
    "    \"\"\"\n",
    "    \"\"\"   \n",
    "    a, b, gamma = sample.numpy()\n",
    "    y = b*np.sin(gamma/180.0*3.1416)\n",
    "    x = b*np.cos(gamma/180.0*3.1416)\n",
    "    tri = Polygon(np.array([[0, 0], [a, 0], [x, y]]), True)\n",
    "    ax.add_patch(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.rand(1000, 3)\n",
    "X[:,0] *= 10\n",
    "X[:,1] *= 10\n",
    "X[:,2] *= 170\n",
    "y = triarea(X[:,0], X[:,1], X[:,2])\n",
    "TRAIN_NUM = 800\n",
    "train_x, test_x = X[:TRAIN_NUM], X[TRAIN_NUM:]\n",
    "train_y, test_y = y[:TRAIN_NUM], y[TRAIN_NUM:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(2,3,i*3+j+1)\n",
    "        draw_sample(X[i*3+j], ax)\n",
    "        ax.axis('off')\n",
    "        ax.set_xlim([0,10])\n",
    "        ax.set_ylim([-5,5])\n",
    "        ax.set_title(\"Area={:.3f}\".format(y[i*3+j].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ArchimedesNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ArchimedesNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(3, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 16)\n",
    "        self.linear4 = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.tanh(self.linear1(x))\n",
    "        h = F.tanh(self.linear2(h))\n",
    "        h = F.tanh(self.linear3(h))\n",
    "        y = nn.functional.relu(self.linear4(h))\n",
    "        return y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ar_net = ArchimedesNet()\n",
    "optim = Adam(ar_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    train_pred = ar_net(train_x).squeeze()\n",
    "    loss = F.mse_loss(train_pred, train_y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if (i % 50 == 0 and i<200) or i%200 == 0:\n",
    "        test_pred = ar_net(test_x).squeeze()\n",
    "        test_loss = F.mse_loss(test_pred, test_y)\n",
    "        print(\"{}: loss {:.2f}, test-loss {:.6f}\".format(i, loss.item(), test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for y0, y1 in zip(test_y, test_pred):\n",
    "    print(\"{:.3f}-{:.3f}\".format(y0.item(), y1.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a few aspects to think about of this example -- this can almost be viewed as a _counter-example_ of statistical learning, where trends are inferred statistically while they are not needed to be so. \n",
    "- but consider what if the semantics of the attributes are not told?\n",
    "- now you are facing a collection of numbers, with respective desirable answers, the above scheme IS a way of extracting trends out of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- and HOPE that the trend generalises - a powerful time-tested line of thought!\n",
    "        \n",
    "    <img src=\"ref/Justus_Sustermans_-_Portrait_of_Galileo_Galilei_1636.jpg\" alt=\"Galileo\" height=\"400\" width=\"300\">\n",
    "        \n",
    ">  Philosophy is written in this grand book, the universe ... It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;....\n",
    "> -- <cite>Galileo, _The Assayer_</cite>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learn to Generalise\n",
    "\n",
    "With the hope, we the greatest challenge / promise of machine learning - to generalise.\n",
    "\n",
    "Let us check our empirical mathematician. First, let's check its response to the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "DOES_TEST_GENERALISE = True\n",
    "X1 = torch.ones(1000, 3)\n",
    "X1[:, 1] = 5.0\n",
    "X1[:, 2] = 90.0 # right angled triangles\n",
    "for i in range(1000):\n",
    "    X1[i, 0] = 0.1 + 0.01*float(i)\n",
    "    if i>900 and DOES_TEST_GENERALISE:\n",
    "        X1[i, 0] += i*0.03\n",
    "y1 = triarea(X1[:,0], X1[:,1], X1[:,2])\n",
    "pred1 = ar_net(X1)\n",
    "\n",
    "X2 = torch.ones(1000, 3)\n",
    "X2[:, 0] = 5.0\n",
    "X2[:, 2] = 90.0\n",
    "for i in range(1000):\n",
    "    X2[i, 1] = 0.1 + 0.01*float(i)\n",
    "    if i>900 and DOES_TEST_GENERALISE:\n",
    "        X2[i, 1] += i*0.01\n",
    "y2 = triarea(X2[:,0], X2[:,1], X2[:,2])\n",
    "pred2 = ar_net(X2)\n",
    "\n",
    "X3 = torch.ones(1000, 3)\n",
    "X3[:, 0] = 3.0\n",
    "X3[:, 1] = 4.0\n",
    "for i in range(1000):\n",
    "    X3[i, 2] = 0.01 + float(i)*175.0/1000\n",
    "y3 = triarea(X3[:,0], X3[:,1], X3[:,2])\n",
    "pred3 = ar_net(X3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X1[:, 0].detach().numpy(), pred1.detach().numpy(), 'b')\n",
    "plt.plot(X1[:, 0].detach().numpy(), y1.detach().numpy(), 'r')\n",
    "plt.legend(['Pred', 'Ground-truth'])\n",
    "plt.xlabel(\"Length: a\")\n",
    "plt.ylabel(\"Area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X2[:, 1].detach().numpy(), pred2.detach().numpy(), 'b')\n",
    "plt.plot(X2[:, 1].detach().numpy(), y2.detach().numpy(), 'r')\n",
    "plt.legend(['Pred', 'Ground-truth'])\n",
    "plt.xlabel(\"Length: b\")\n",
    "plt.ylabel(\"Area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X3[:, 2].detach().numpy(), pred3.detach().numpy(), 'b')\n",
    "plt.plot(X3[:, 2].detach().numpy(), y3.detach().numpy(), 'r')\n",
    "plt.legend(['Pred', 'Ground-truth'])\n",
    "plt.xlabel(\"Length: b\")\n",
    "plt.ylabel(\"Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: The true world model is, of course, $$\n",
    "A = \\frac{1}{2} \\sin(\\gamma) * a * b\n",
    "$$\n",
    "However, can you consider any generalisability issue for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from Data Framework\n",
    "\n",
    "\n",
    "<img src=\"ref/learning.png\" alt=\"LearningFramework\" height=\"400\" width=\"500\">\n",
    "\n",
    "The major players:\n",
    "- data\n",
    "- models (hypotheses)\n",
    "- algorithm\n",
    "- selection criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Let's understand the data first!__\n",
    "\n",
    "<span style=\"color:blue\">__DATA__:$\\mathcal{D}$</span>\n",
    "\n",
    "If we rip off all \"semantics/domain-specific interpretations/conceptual icing-on-the-cake-of-theory\", each object of interest boils down to a bunch of numbers in any analytics on a digital computer ([Cool works](https://www.youtube.com/watch?v=Ecvv-EvOj8M) have revealed evidence supportting the brain works the similar way, too!). \n",
    "\n",
    "We call such a collection of numbers, a _sample_. Each of the number is an _attribute_. Say, each sample contains $p$ attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some alternative names of the concepts implies interesting views of the data. \n",
    "- Alternatively, samples are called data points or sample points. It is not hard to imagine that \"point\" alludes to the geometric understanding of the data as living in a $p$-dimensional space, each sample corresponding to a point in the space. And the point is determined by its coordinates which are just the numbers making that sample. \n",
    "- Note an attribute is more generic than a number in a particular sample -- it refers to that number across all possible samples. We can take an attribute-centric view of the data, where each sample is an instance of the $p$ _random variables_.\n",
    "\n",
    "The two views (geometric and stochastic) of the data are non-exclusive. For example, you can think of the data to be analysed as some cloudy stuff spreading (distribution) across the $p$-dimensional space. While the distribution is both unknown and difficult to describe (even if we think for a while it were known), all the analyser can get her hands on is a number of points scattered within the \"distribution cloud\", and the points are our sample points. Here our adventure begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: what is the difference between training and test data in the above example (or if you are familier with the area, in general)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: can you talk about what makes a data model?\n",
    "\n",
    "In practical analytics, when we are talking about _models_, we actually operates with another concept -- _model family_. Each model is a very particular theory about the relationship between the attributes of samples and the corresponding targets. Such a theory is called a _hypothesis_. However, any particular hypothesis may of little use in practice: say you have a linear model with $w_1=0.1573, w_2=-1.2856$ and $b = -0.0021$, for some data of two attributes, it is hard to imagine the model is optimal for any practical 2D data analytic task. So in the business of data analytics, we adopt the _learning_ approach -- where instead of focusing on an individual data model, we define a _model family_, which consists of many, often infinitely many, hypothesis. \n",
    "\n",
    "<span style=\"color:blue\">__HYPOTHESIS SPACE__:$\\mathcal{H}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Targets__\n",
    "\n",
    "Recall our question above. The target of analytics should be given for sample examples. This piece of information, or the lack of it, defines many different types of machine learning. If you think about the situation carefully, there is no essential distinction between \"targets\" or \"attributes\" from the point of view of an object. The distinction is made on the model learner's side. Targets are a special set of attributes, which are accessible during the stage we adjust our model. Once the model has been fixed and deployed, the \"target\" variables are no longer available to the model, for its namesake, they becomes the \"target\" of the model prediction. \n",
    "\n",
    "From the various ways the targets are given (or missing) arise supervised (we will see shortly), unsupervised, reinforcement, transfer, ..., learning schemes.\n",
    "\n",
    "Given the hypotheses in some $\\mathcal{H}$, and in the light of data samples drawn from $\\mathcal{D}$, we perform some process to pick up the one that mostly fits. The process is called _learning_.\n",
    "\n",
    "<span style=\"color:blue\">__LEARNING ALGORITHM__:$\\mathcal{A}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The purpose of learning is of course to make the model's prediction on the targets, given the observable attributes, better aligned with that of the true data. Technically, we need a single criterion, you can consider it as the \"KPI\" of the model. We optimise over the model parameters (i.e. picking up a specific model/hypothesis from $\\mathcal{H}$) to have the best KPI-measurement on the _training set_ of data. By convention, the criterion is often formulated as the discrepancy between the desired target and the model prediction, which is to be _minimised_.\n",
    "\n",
    "<span style=\"color:blue\">__LOSS__:$\\mathcal{L}$</span>.\n",
    "\n",
    "By now we have encountered all elements in learning-based machine intelligence $(\\mathcal{D}, \\mathcal{H}, \\mathcal{L}, \\mathcal{A})$ -- if you accept the view of intelligence as \"capable of summarising from past experience to shape behaviour for future reward\". \n",
    "\n",
    "__Q__\n",
    "Can you identify a key assemption in the entire formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Knowledge Mindmap\n",
    "I provide a knowledge map [here](https://sketchboard.me/xA4SQKJWSZcd#/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXAMPLE-HAND-WRITTEN DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DDIR = os.path.expanduser(\"~/data/common\")\n",
    "if not os.path.exists(DDIR):\n",
    "    os.makedirs(DDIR)\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = torchvision.datasets.MNIST(\n",
    "    root=DDIR, train=True, download=True,\n",
    "    transform=mnist_transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(\n",
    "    mnist_trainset, batch_size=32,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "mnist_testset = torchvision.datasets.MNIST(\n",
    "    root=DDIR, train=False, download=True,\n",
    "    transform=mnist_transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(\n",
    "    mnist_testset, batch_size=32\n",
    ")\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    npimg -= npimg.min()\n",
    "    npimg /= npimg.max()\n",
    "    if npimg.shape[0] in [3,4]:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    else:\n",
    "        plt.imshow(npimg.squeeze(), interpolation='nearest', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in mnist_trainloader:\n",
    "    break\n",
    "show(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us construct a _convolutional_ neural network for the image data. Here we have a [nice animation](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) for intuitive interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p_ = list(conv.parameters())\n",
    "print(\"There are {} parameter objects.\".format(len(p_)))\n",
    "print(p_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_[0][0, 0, 0, 0] = 1.0\n",
    "p_[0][0, 0, 0, 1] = 1.0\n",
    "p_[0][0, 0, 0, 2] = 1.0\n",
    "p_[0][0, 0, 1, 0] = 0.0\n",
    "p_[0][0, 0, 1, 1] = 0.0\n",
    "p_[0][0, 0, 1, 2] = 0.0\n",
    "p_[0][0, 0, 2, 0] = -1.0\n",
    "p_[0][0, 0, 2, 1] = -1.0\n",
    "p_[0][0, 0, 2, 2] = -1.0\n",
    "print(p_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the effect of the operation of `conv` on our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "h = conv(x)\n",
    "show(x[0])\n",
    "plt.show()\n",
    "show(h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, the point of _learning_ is to automatically discover meaningful processings (recall our old friend empiricist \"mathematician\"), and to a much more complext level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class HandwrittenDigitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandwrittenDigitNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=3)\n",
    "        self.linear = nn.Linear(3200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: a batch of images\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.conv2(h)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.linear(h.view(h.shape[0], 3200))\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mnist_net = HandwrittenDigitNet()\n",
    "mnist_optim = Adam(mnist_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "last_test_accu = 0.1\n",
    "small_improve_evals = 0\n",
    "max_epoch = 100\n",
    "max_small_imporve_evals = 20\n",
    "evaluate_every_n_steps = 10\n",
    "stop_cond = False\n",
    "while not stop_cond:\n",
    "    it = 0\n",
    "    for X, y in mnist_trainloader:\n",
    "        mnist_optim.zero_grad()\n",
    "        h = mnist_net(X)\n",
    "        loss = F.nll_loss(h, y)\n",
    "        loss.backward()\n",
    "        mnist_optim.step()\n",
    "        it += 1\n",
    "        if it % evaluate_every_n_steps==0:\n",
    "            correct_num = 0\n",
    "            for Xt, yt in mnist_testloader:\n",
    "                test_h = mnist_net(Xt)\n",
    "                test_c = torch.argmax(test_h, dim=1)\n",
    "                correct_num += torch.sum(test_c == yt).item()\n",
    "            test_accu = float(correct_num) / len(mnist_testset)\n",
    "            imporve_rate = (1.0-last_test_accu) / (1.0-test_accu) - 1.0\n",
    "            if imporve_rate < 0.05:\n",
    "                small_improve_evals += 1\n",
    "            else:\n",
    "                small_improve_evals = 0\n",
    "            last_test_accu = max(test_accu, last_test_accu)\n",
    "            print(\"Epoch {}, iteration {} \"\n",
    "                  \"train loss {:.3f} test accuracy {:.3f} \"\n",
    "                  \"imp {:.3f} no-improve-steps {}\".format(\n",
    "                      epoch, it, loss, test_accu, \n",
    "                      imporve_rate,\n",
    "                      small_improve_evals))\n",
    "            if small_improve_evals >= max_small_imporve_evals:\n",
    "                stop_cond = True\n",
    "                break\n",
    "                \n",
    "    epoch += 1\n",
    "    stop_cond = stop_cond or epoch >= max_epoch\n",
    "    # you can validate the model on test data here, try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXAMPLE CYCLE-GAN\n",
    "\n",
    "We can expand the above scheme to a much larger scale.\n",
    "- many more layers\n",
    "- output many more answers for each data sample -- e.g. \n",
    "> we can output a \"fake\" image, entirely produced by the data model, while each pixel of the fake image is an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/cgan.png\" alt=\"CGAN\" height=\"300\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import utils.cganimstyler as cim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "AVAILABLE_TARGET_STYLES = [\n",
    "    \"apple2orange\", \"orange2apple\", \n",
    "    \"summer2winter_yosemite\", \"winter2summer_yosemite\", \n",
    "    \"horse2zebra\", \"zebra2horse\", \"monet2photo\", \n",
    "    \"style_monet\", \"style_cezanne\", \"style_ukiyoe\", \n",
    "    \"style_vangogh\", \"sat2map\", \"map2sat\", \n",
    "    \"cityscapes_photo2label\", \"cityscapes_label2photo\", \n",
    "    \"facades_photo2label\", \"facades_label2photo\", \"iphone2dslr_flower\"\n",
    "]\n",
    "\n",
    "TARGET_STYLE = AVAILABLE_TARGET_STYLES[10]\n",
    "print(\"TARGET_STYLE: \", TARGET_STYLE)\n",
    "# download trained style-conversion models\n",
    "model_path = \"checkpoints/saved_style_models/\" + TARGET_STYLE + \".pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/\" + \\\n",
    "        TARGET_STYLE + \".pth\",\n",
    "        model_path)\n",
    "\n",
    "# build the style model\n",
    "netG = cim.load_generator_from(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "im = cim.load_image('data/Jun.jpeg') # Put your own image here!\n",
    "res = netG(im)\n",
    "npim = cim.tensor2im(im)\n",
    "res_npim = cim.tensor2im(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(npim)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(res_npim)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXAMPLE A3C\n",
    "\n",
    "This represents a family of algorithms simultaneously collect experience while training a reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Definition\n",
    "We build a program that can play video games:\n",
    "```\n",
    "Input: Game-Environment (env)\n",
    "Output: Game-Policy (policy)\n",
    "```\n",
    "\n",
    "__env__:\n",
    "```\n",
    "Input: action (0~k, say, 2)\n",
    "Output: screen-image, reward, game-is-over\n",
    "```\n",
    "\n",
    "__policy__:\n",
    "```\n",
    "Input: screen-image\n",
    "Output: action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is not difficult to imagine how a policy “plays” an env. The goal is to design the policy self-inspection and adjustment scheme (call this meta-policy if you like), so the total reward it receives in a game maximises. Note this setting can be more generic than you might think of:\n",
    "- the game can give a reward of any constant positive value at each step to simply encourage the player to stay playing as long as possible, which makes sense in some balancing or jumpping games. In some games that can even mean disencourage winning, such as in ball games!\n",
    "- the game can give a reward of any constant negative value at each step to encourage quick playing, such as in a maze game without suicidal option, this is equal to saying \"hurry up!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building Blocks\n",
    "\n",
    "1. Neural Networks\n",
    "    1. Network building using torch\n",
    "    2. Network training using gradient descent\n",
    "        1. Compute gradients with back propagation\n",
    "        2. Commit parameter update along gradient direction\n",
    "        \n",
    "2. Reinforcement Learning Algorithms\n",
    "    1. Long-term evaluation of actions -- building a slot-machine playing agent\n",
    "    2. Handling machines with internal states\n",
    "    3. Handling machines with MANY internal states\n",
    "        1. Using neural networks to estimate action values\n",
    "        2. Adjust action probability by reviewing consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technical Terms\n",
    "\n",
    "A small number of technical terms are used in our discussion. One way to treat a strange jargon is just ignore it when encountering, and let its meaning emerge by itself during your study. If you find your short-term memory is going to explode because the need to keep track many strange notions -- it might be helpful to look up in a glossary such as [here](https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/). Or simply Google the new concept. However, I am afraid google/wiki-def of the notion can only partially help your study -- the key is the fact that many new concepts need to be learned, so only careful review can help achieve a deep understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Math Glossary\n",
    "Some math representation of useful concepts are:\n",
    "\n",
    "- $s_t$: the state observed at time $t$. This is usually but NOT always the stuff returned to the agent at the time taking actions. The most prominent exception is the screen-image-based states in our video-game playing examples. One applies some simple preprocessing to the states.\n",
    "\n",
    "- $a_t$: the action taken at time $t$. Generally, it is an integer $\\{0, 1, ..., K-1\\}$ if there are $K$ different actions. Keep in mind that the actual action could be represented differently, such as \"press A-button\". For a decision making agent, given all possible action choices, choosing actions is eqivalent to choosing the indexes.\n",
    "\n",
    "- $r_t$: the immediate reward received at time $t$. Note some authors used to let $r_t$ refer to the reward received __after__ taking action $a_t$ in state $s_t$, while others take $r_t$ as the reward received __at the beginning__ at time $t$, after taking action $a_{t-1}$ in state $s_{t-1}$. In whatever way, the procedure: in $s_t$ taking action $a_t$ according to some policy $\\pi$ arriving the next state $s_{t+1}$ and receiving a reward $r_{t}$ (or $r_{t+1}$ subject to your choice of denotation) is called a __transition step__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\pi(\\cdot|s)$, given the state $s$, a policy $\\pi(a|s)$ (not to be confused with the $\\pi\\approx3.1416$) assigns a non-negative real number to each action -- it dictates the possibility of choosing the action given $s$. If a policy is deterministic, rather than stochastic, it can attribute all probabilities to one particular action, so that the corresponding $\\pi(a|s)=1$ and $\\pi(a'|s)=0$ for all other actions $a'$.\n",
    "\n",
    "- $Q^\\pi(s, a)$, evaluation of __long term__ return for taking action $a$ in state $s$. Since it considers future effects, it relies on the on-going policy, $\\pi$. Note taking $a$ at the current state $s$, the very first step of this evaluation is not necessarily with respect to $\\pi$. Consider this $Q$-evaluation as answering a hypothetical question: what the long term reward would have been if she took action $a$ at $s$ and followed $\\pi$ henceafter. Of course if this evaluation is known, it is wise to take the action maximising this $Q$ at each $s$.\n",
    "\n",
    "- Note in neural network implementation, $Q$- and $\\pi$-nets share the same structure: map states to $K$ numbers, where $K$ is the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from utils.a3c import a3c\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pong-v0')\n",
    "s = env.reset()\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def a3c_test(env, model):\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    reward_sum = 0\n",
    "    done = True\n",
    "\n",
    "    # a quick hack to prevent the agent from stucking\n",
    "    episode_length = 0\n",
    "    cx = hx = None\n",
    "    while True:\n",
    "        episode_length += 1\n",
    "        # Sync with the shared model\n",
    "        if done:\n",
    "            with torch.no_grad():\n",
    "                cx = torch.zeros(1, 256)\n",
    "                hx = torch.zeros(1, 256)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                cx = cx.detach()\n",
    "                hx = hx.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            s_ = state.unsqueeze(0)\n",
    "        value, logit, (hx, cx) = model((s_, (hx, cx)))\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "        # print logit.data.numpy()\n",
    "        action = prob.max(1, keepdim=True)[1].data.cpu().numpy()\n",
    "\n",
    "        state, reward, done, _ = env.step(action[0, 0])\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        done = done or episode_length >= 10000\n",
    "        reward_sum += reward\n",
    "\n",
    "        # a quick hack to prevent the agent from stucking\n",
    "        # actions.append(action[0, 0])\n",
    "        # if actions.count(actions[0]) == actions.maxlen:\n",
    "        #     done = True\n",
    "\n",
    "        if done:\n",
    "            print(\"Reward {}, episode length {}\".\n",
    "                  format(reward_sum, episode_length))\n",
    "            env.close()\n",
    "            break\n",
    "        state = torch.from_numpy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A3C_CHECKPOINTS = [40, 100, 200]\n",
    "A3C_CP = 2\n",
    "\n",
    "model = a3c.ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "if A3C_CP >= 0:\n",
    "    checkpoint = torch.load('checkpoints/a3c_models/PongDeterministic-v4_worker2_{}'.format(A3C_CHECKPOINTS[A3C_CP]))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = a3c.create_atari_env('PongDeterministic-v4')\n",
    "env.reset()\n",
    "a3c_test(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# EXAMPLE - YOLO\n",
    "\n",
    "This section is under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The example follows the [Yolo Tutorial from paperspace][1].\n",
    "\n",
    "[1]:https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\n",
    "\n",
    "#### Understanding YOLO-Network output and detection task\n",
    "\n",
    "Each pixel in the final convolutional layer has $B\\times(5+C)$ entries. $B$ is the number of bounding boxes each cell can predict. Each of the box is for detecting a certain kind of object. Each bounding box has $5+C$ attributes -- centre coordinates (x, y, offset?), dimensions (width, height), objectness score and $C$ classes confidence. __Q__: if each of the $B$  bounding box is corresponding to one certain kind of object, then why each bounding box can have $C$ class likelihood?\n",
    "\n",
    "Each cell can predict an object in one of it bounding boxes _if the centre of the object falls in the receptive field of the cell_.\n",
    "\n",
    "For training (or computing the training loss), the input image is divided according to the final feature map -- if the final feature map represents a 32x shrink of the image, the image will be divided by $32 \\times 32$ grids. See the picture below <img src=\"ref/yolo-5.png\" alt=\"Smiley face\" height=\"400\">\n",
    "\n",
    "Each cell defines $B$ anchors (default boxes). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Network outputs $t_x, t_y, t_w, t_h$. For $x$-coordinate, $b_x = \\sigma(t_x)+c_x$: sigmoid to predict offset in $x$ direction with respect to grid-cell-centre $c_x$. \n",
    "\n",
    "> For example, consider the case of our dog image. If the prediction for center is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).\n",
    "\n",
    "$b_w = e^{t_w}$ -- the width of the box. $1.0$ is the edge length of a cell.\n",
    "\n",
    "Anyway, each cell of the image is thought as of $1.0 \\times 1.0$.\n",
    "\n",
    "> The resultant predictions, bw and bh, are normalised by the height and width of the image. (Training labels are chosen this way). So, if the predictions bx and by for the box containing the dog are (0.3, 0.8), then the actual width and height on 13 x 13 feature map is (13 x 0.3, 13 x 0.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Understanding YOLO -- Processing Steps\n",
    "\n",
    "Processing and downsampling, until to the $32\\times 32$ cell, then upsample and use skip-layer connections. 32, 16, 8 are used.\n",
    "\n",
    "[IDEA] can we use high-resolution image, rather than upsampling for fine-grain detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Epilogue\n",
    "\n",
    "\n",
    "<img src=\"ref/Frans_Hals-Descartes.jpg\" width=\"200\" height=\"400\">\n",
    "\n",
    "> Cogito, ergo sum\n",
    "\n",
    "    -- René Descartes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
